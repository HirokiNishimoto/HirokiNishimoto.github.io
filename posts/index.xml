<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on にっしーの備忘録</title>
    <link>https://hirokinishimoto.github.io/posts/</link>
    <description>Recent content in Posts on にっしーの備忘録</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 02 Nov 2019 21:41:41 +0900</lastBuildDate>
    
	<atom:link href="https://hirokinishimoto.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>[論文]Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to &#43;1 or −1</title>
      <link>https://hirokinishimoto.github.io/posts/bnn/</link>
      <pubDate>Sat, 02 Nov 2019 21:41:41 +0900</pubDate>
      
      <guid>https://hirokinishimoto.github.io/posts/bnn/</guid>
      <description>論文pdf
背景  Deep Learningの学習はエネルギー消費の多いGPUを使うことがほとんど。 近年low-powerなデバイス上でDeepLearningの計算を行うための研究も盛んに行われている。  Binarized Neural Networks (BNNs) 著者は重みだけでなく活性も2値化したBinarized Neural Networkを提案した。
これは、学習時は2値化された重みと活性を使って勾配計算し、2値化された重みと活性を用いて推論するようなNeural Networkのことである。
重みの2値化 決定的(Deterministic)な方法と、確率的(Stochastic)な方法がある。
 Deterministicな方法: $$ w_{b}=\left{\begin{array}{ll}{+1} &amp;amp; {\mathrm {if}\ w \geq 0} \ {-1} &amp;amp; {\text { otherwise }}\end{array}\right. $$
 Stochasticな方法 $$ w_{b}=\left{\begin{array}{ll}{+1} &amp;amp; {\mathrm { with\ probability }\ p=\sigma(w)} \ {-1} &amp;amp; {\mathrm { with\ probability }\ 1-p}\end{array}\right. $$
  $$ \mathrm{where}\ \sigma(x)=\operatorname{clip}\left(\frac{x+1}{2}, 0,1\right)=\max \left(0, \min \left(1, \frac{x+1}{2}\right)\right) $$ ( \sigma(x))はハードシグモイド関数である。 Stochasticな2値化は乱数発生器を必要とするので実装がやや困難である。 この論文においては、活性化に対してはいくつかの実験で学習時にStochastiな2値化を行うが、それ以外は専らDeterministicな2値化を行う。</description>
    </item>
    
    <item>
      <title>MatplotlibのグラフをRetinaの高解像度で表示する方法</title>
      <link>https://hirokinishimoto.github.io/posts/matplotlib%E3%81%AE%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92retina%E3%81%AE%E9%AB%98%E8%A7%A3%E5%83%8F%E5%BA%A6%E3%81%A7%E8%A1%A8%E7%A4%BA%E3%81%99%E3%82%8B%E6%96%B9%E6%B3%95/</link>
      <pubDate>Sat, 02 Nov 2019 21:13:27 +0900</pubDate>
      
      <guid>https://hirokinishimoto.github.io/posts/matplotlib%E3%81%AE%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92retina%E3%81%AE%E9%AB%98%E8%A7%A3%E5%83%8F%E5%BA%A6%E3%81%A7%E8%A1%A8%E7%A4%BA%E3%81%99%E3%82%8B%E6%96%B9%E6%B3%95/</guid>
      <description>この記事が参考になった。
%config InlineBackend.figure_format = &amp;#39;retina&amp;#39;</description>
    </item>
    
    <item>
      <title>AIC</title>
      <link>https://hirokinishimoto.github.io/posts/aic/</link>
      <pubDate>Tue, 29 Oct 2019 12:15:01 +0900</pubDate>
      
      <guid>https://hirokinishimoto.github.io/posts/aic/</guid>
      <description>平均対数尤度: $E(\log{L})$ モデルの最大対数尤度: $\log{L^*}$ モデルの自由度: $k$  としたとき、 $$ E(\log{L}) = \log{L}^* - k$$ の関係が成り立つと緑本に書いてあった。 ポアソン回帰の例でこの関係が本当に成り立つのか実験してみた。
ライブラリの読み込み import numpy as np import pandas as pd import matplotlib.pyplot as plt import scipy.stats as stats import japanize_matplotlib import seaborn as sns import statsmodels.api as sm 実験 bias1 = [] bias2 = [] n_rep = 100 size = 50 n_newdata = 100 for i in range(n_rep): # データの発生 x = np.random.randint(0,100,size=size) y = np.</description>
    </item>
    
    <item>
      <title>Rust入門</title>
      <link>https://hirokinishimoto.github.io/posts/rust%E5%85%A5%E9%96%80/</link>
      <pubDate>Wed, 16 Oct 2019 16:00:21 +0900</pubDate>
      
      <guid>https://hirokinishimoto.github.io/posts/rust%E5%85%A5%E9%96%80/</guid>
      <description>Rust Rustチュートリアル</description>
    </item>
    
    <item>
      <title>C&#43;&#43;の標準データ構造</title>
      <link>https://hirokinishimoto.github.io/posts/c&#43;&#43;%E3%81%AE%E6%A8%99%E6%BA%96%E3%83%87%E3%83%BC%E3%82%BF%E6%A7%8B%E9%80%A0/</link>
      <pubDate>Mon, 14 Oct 2019 22:45:17 +0900</pubDate>
      
      <guid>https://hirokinishimoto.github.io/posts/c&#43;&#43;%E3%81%AE%E6%A8%99%E6%BA%96%E3%83%87%E3%83%BC%E3%82%BF%E6%A7%8B%E9%80%A0/</guid>
      <description>stack LIFOのデータ構造
メンバ関数  push() : 要素の挿入 top() : 先頭の要素の参照 pop() : 先頭の要素の破棄 size() : 現在の要素数  計算コストは全てO(1)
練習問題
queue FIFOのデータ構造
メンバ関数  push() : 要素の挿入 front() : 先頭の要素の参照 pop() : 先頭の要素の破棄 size() : 現在の要素数  計算コストは全てO(1)
練習問題
priority_queue 挿入されたデータの中で最大の要素の参照が低数時間でできる。 実装には2分ヒープなど
メンバ関数  push() : 要素の挿入 top() : 先頭の要素の参照 pop() : 先頭の要素の破棄 size() : 現在の要素数  計算コストはtop() : O(1), push(), pop() : O(logN)
練習問題
set 集合を表現するライブラリ
種類 要素の重複を許さないもの  set : データに順序をつけて管理</description>
    </item>
    
    <item>
      <title>RでSEM</title>
      <link>https://hirokinishimoto.github.io/posts/r%E3%81%A7sem/</link>
      <pubDate>Mon, 14 Oct 2019 01:07:22 +0900</pubDate>
      
      <guid>https://hirokinishimoto.github.io/posts/r%E3%81%A7sem/</guid>
      <description>はじめに 本稿では、トイデータを使って、Rで共分散構造分析を行う方法について解説する。
状況 例えば、下のようなモデルが下のトイデータに整合するかを共分散構造分析で検証したいとする。
トイデータ # データの読み込み dat &amp;lt;- matrix(c( + -2.11, -1.12, 1.42, 2.23, 1.53, + 0.06, 1.81, -0.59, -0.75, 10.12, + 2.58, -0.20, -1.92, -0.49, -0.35, + 0.69, -0.66, -0.77, -1.92, 0.38, + -1.05, 0.07, -0.37, -0.89, -1.62, + -2.73, 1.40, 0.18, -0.09, 0.13, + 0.95, 0.84, 1.19, 1.19, 0.10, + 0.93, 1.17, -1.70, 1.46, -0.25, + -0.04, -0.12, -0.34, -0.24, 1.88, + 0.16, 1.03, -0.05, -0.73, 0.</description>
    </item>
    
    <item>
      <title>Hello Wor;d!</title>
      <link>https://hirokinishimoto.github.io/posts/binaryconnect/</link>
      <pubDate>Wed, 02 Oct 2019 21:35:14 +0900</pubDate>
      
      <guid>https://hirokinishimoto.github.io/posts/binaryconnect/</guid>
      <description>[論文]BinaryConnect: Training Deep Neural Networks with binary weights during propagations 論文pdf
背景 近年、計算可能性が新しいアルゴリズム開発のネックになる場面が散見されるようになった。また、実際の応用において、GPUよりもパワーの弱いデバイスにDeepLearningのモデルを載せたいというニーズも出てきた。そこで、DeepLearningの学習、予測の際の計算高速化に関心が集まっている。
モデルの重みを2値化することで、積和操作を単なる和に置き換える事ができる。乗算器は、Deep Learning の計算において、メモリとパワーを大変多く消費するので、これは利点が大きい。
著者は、重みの2値化によるDeepLearningの計算高速化の手法の1つとして、BinaryConnect(BC)を提案した。
BinaryConnect 重みの2値化 決定的(Deterministic)な方法と、確率的(Stochastic)な方法がある。
Deterministicな方法: $$ w_{b}=\left{\begin{array}{ll}{+1} &amp;amp; {\mathrm {if}\ w \geq 0} \ {-1} &amp;amp; {\text { otherwise }}\end{array}\right. $$
Stochasticな方法 $$ w_{b}=\left{\begin{array}{ll}{+1} &amp;amp; {\mathrm { with\ probability }\ p=\sigma(w)} \ {-1} &amp;amp; {\mathrm { with\ probability }\ 1-p}\end{array}\right. $$
$$ \mathrm{where}\ \sigma(x)=\operatorname{clip}\left(\frac{x+1}{2}, 0,1\right)=\max \left(0, \min \left(1, \frac{x+1}{2}\right)\right) $$
( \sigma(x))はハードシグモイド関数である。ソフトなシグモイド関数を用いなかったのは、計算量を減らすためである。
学習時  順伝播、逆伝播の際は2値化された重みを用いる。</description>
    </item>
    
    <item>
      <title>Hello Wor;d!</title>
      <link>https://hirokinishimoto.github.io/posts/hello/</link>
      <pubDate>Wed, 02 Oct 2019 21:35:14 +0900</pubDate>
      
      <guid>https://hirokinishimoto.github.io/posts/hello/</guid>
      <description>このサイトはにっしーの備忘録に使われます.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://hirokinishimoto.github.io/posts/binarizedneuralnetwoeks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hirokinishimoto.github.io/posts/binarizedneuralnetwoeks/</guid>
      <description>[論文]Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1 論文pdf
背景  Deep Learningの学習はエネルギー消費の多いGPUを使うことがほとんど。 近年low-powerなデバイス上でDeepLearningの計算を行うための研究も盛んに行われている。  Binarized Neural Networks (BNNs) 著者は重みだけでなく活性も2値化したBinarized Neural Networkを提案した。
これは、学習時は2値化された重みと活性を使って勾配計算し、2値化された重みと活性を用いて推論するようなNeural Networkのことである。
重みの2値化 決定的(Deterministic)な方法と、確率的(Stochastic)な方法がある。
 Deterministicな方法: $$ w_{b}=\left{\begin{array}{ll}{+1} &amp;amp; {\mathrm {if}\ w \geq 0} \ {-1} &amp;amp; {\text { otherwise }}\end{array}\right. $$
 Stochasticな方法 $$ w_{b}=\left{\begin{array}{ll}{+1} &amp;amp; {\mathrm { with\ probability }\ p=\sigma(w)} \ {-1} &amp;amp; {\mathrm { with\ probability }\ 1-p}\end{array}\right.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://hirokinishimoto.github.io/posts/mobilenets/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hirokinishimoto.github.io/posts/mobilenets/</guid>
      <description>[論文]MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications 論文pdf
Mobilenetの特徴  軽い 早い 精度が良い  Depthwise Separable Convolution Mobile Netの軽量化のための工夫。
 わかりやすい動画
https://www.youtube.com/watch?v=T7o3xvJLuHk&amp;amp;feature=youtu.be
 MobileNetの全体の構造 1層目だけFull Convolution層。 2層目以降はdepthwise separable convolutionの繰り返し。
 積-和演算の数を減らせる 残りの積-和計算を効率的に実装できる。  パラメータの75%、計算時間の95%を1x1畳込み層が占めている。
1x1畳込み層は、線形代数の数値計算が最適化されたアルゴリズムの１つであるgeneral matrix multiply (GEMM) functionでdirectに実装可能。(???)
Hyper-parameters  width multiplier
 論文では( \alpha )という記号で表されている。 ( 0 \leq \alpha \leq 1 ) の範囲で制御可能 (\alpha)が小さくなるとinput channel, output channelが(\alpha)倍小さくなる。 計算コストは大体(\alpha^2)に accuracyとのトレードオフがある  resolution multiplier
 論文では( \rho )という記号で表されている。 ( 0 \leq \rho \leq 1 ) の範囲で制御可能 (\rho)が小さくなるとfeature mapの大きさが(\rho)倍小さくなる。 計算コストは(\rho^2)倍小さくなる。 (\rho)を変化させてもパラメータ数は変化しない。 accuracyとのトレードオフがある   ハイパーパラメータを導入すると、Depthwise Separable Convolution の計算にかかるコストは</description>
    </item>
    
    <item>
      <title></title>
      <link>https://hirokinishimoto.github.io/posts/squeezenet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hirokinishimoto.github.io/posts/squeezenet/</guid>
      <description>[論文]SQUEEZENET: ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND &amp;lt;0.5MB MODEL SIZE 論文pdf
背景 小さなCNNアーキテクチャの利点  より効率的な分散学習 新しいモデルをclientにexportする際のoverheadを減らせる  より頻繁なモデルの更新も可能に  FPGAなど、メモリが限られたハードウェア上により柔軟にDeployできる  本論文のContribution  従来研究は高精度なモデルを目指したものが多い 本論文はCNNの設計空間を探索して、性能が他に劣らずかつ小さなSqueezeNetを発見、提案  ImageNetに対して、Alex-Netと同等の正答率 Alex-Netよりも50倍少ないパラメータ数 モデル圧縮により、SqueezeNetを0.5MBまで圧縮した  Alex-Netより510倍小さい    CNN設計空間の探索  探索を自動化しなかった → 設計空間の形に対するintuitionを得たい  CNNアーキテクチャの構成要素  microarchitecture &amp;gt; layer, moduleの構造
 macroarchitecture &amp;gt; 入力から出力まで全体の構造
&amp;gt; microarchitectureの接続の仕方
 solver &amp;gt; 論文では具体的な指定なし
&amp;gt; 実装ではAdagradが使われがち
 hyperpalameters
  CNN設計空間の探索の方針 1. 3x3Filterを1x1Filterに置き換える 2. 3x3Fileterへのinput channelの数を減らす 3.</description>
    </item>
    
  </channel>
</rss>